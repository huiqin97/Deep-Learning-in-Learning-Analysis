{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1h4Dnjr5dS90CsCXXDyx9GDkCBpdXkb5-",
      "authorship_tag": "ABX9TyOb3BQV8QQ0la60ctberfrP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/huiqin97/Deep-Learning-in-Learning-Analysis/blob/main/FYP_LSTM_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "KjygU_2nb2Xc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stInfo  = pd.read_csv('https://raw.githubusercontent.com/huiqin97/Deep-Learning-in-Learning-Analysis/main/studentInfo.csv')\n",
        "stVLE  = pd.read_csv('/content/drive/MyDrive/studentVle.csv')\n",
        "VLE  = pd.read_csv('https://raw.githubusercontent.com/huiqin97/Deep-Learning-in-Learning-Analysis/main/vle.csv')\n",
        "# df_assessment = pd.read_csv('https://raw.githubusercontent.com/huiqin97/Deep-Learning-in-Learning-Analysis/main/assessments.csv')\n",
        "stAss  = pd.read_csv('https://raw.githubusercontent.com/huiqin97/Deep-Learning-in-Learning-Analysis/main/studentAssessment.csv')\n",
        "# df_course_details = pd.read_csv('https://raw.githubusercontent.com/huiqin97/Deep-Learning-in-Learning-Analysis/main/courses.csv')\n",
        "stReg  =  pd.read_csv('https://raw.githubusercontent.com/huiqin97/Deep-Learning-in-Learning-Analysis/main/studentRegistration.csv')"
      ],
      "metadata": {
        "id": "fv_WHmknb9a6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the data of student who dropout before courses start\n",
        "stVLE_update = stVLE[stVLE['date']>=0]"
      ],
      "metadata": {
        "id": "XA-t_7MVcWwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract pass, disctinctio and fail students\n",
        "stInfo_update = stInfo[(stInfo['final_result'] =='Pass') | (stInfo['final_result'] =='Distinction')\n",
        "                       | (stInfo['final_result'] =='Fail')] "
      ],
      "metadata": {
        "id": "-nnUF15ucn-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "courses = stInfo_update['code_module'].unique()\n",
        "dic_site_acitivity = {v:k for v, k in zip(VLE['id_site'], VLE['activity_type'])}\n",
        "actions_len = len(VLE['activity_type'].unique())\n",
        "\n",
        "actions_dic = VLE['activity_type'].unique().tolist()\n",
        "\n",
        "actions_dic = {v: k for k, v in enumerate(actions_dic)}\n",
        "dic_acitivity_idx = {v:k for v, k in actions_dic.items()}\n",
        "\n",
        "print(dic_acitivity_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQIMi0Qccr_r",
        "outputId": "c7ea9908-6fc5-4ff1-cbdc-64252feb2b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'resource': 0, 'oucontent': 1, 'url': 2, 'homepage': 3, 'subpage': 4, 'glossary': 5, 'forumng': 6, 'oucollaborate': 7, 'dataplus': 8, 'quiz': 9, 'ouelluminate': 10, 'sharedsubpage': 11, 'questionnaire': 12, 'page': 13, 'externalquiz': 14, 'ouwiki': 15, 'dualpane': 16, 'repeatactivity': 17, 'folder': 18, 'htmlactivity': 19}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features for the course  BBB, 2013 as the training and 2014 as the test\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder = LabelEncoder()\n",
        "\n",
        "labelencoder.fit([\"Distinction\", \"Fail\", \"Pass\"])\n",
        "\n",
        "courses = ['BBB'] # we can also add AAA, BBB, CCC\n",
        "\n",
        "for i, course in enumerate(courses):\n",
        "    \n",
        "    stVLE_subset = stVLE_update[stVLE_update['code_module'] == course]\n",
        "    stInfo_update_filtered = stInfo_update[stInfo_update['code_module'] == course]\n",
        "    \n",
        "    # filter multiple attemps\n",
        "    stInfo_update_filtered.drop_duplicates(subset=['id_student'], keep=False, inplace=True)\n",
        "    \n",
        "    stVLE_subset_filtered = stVLE_subset[stVLE_subset['id_student'].isin(stInfo_update_filtered['id_student'])]\n",
        "    \n",
        "    stInfo_update_filtered = stInfo_update_filtered[stInfo_update_filtered['id_student'].isin(stVLE_subset_filtered['id_student'])]\n",
        "    \n",
        "    \n",
        "    stVLE_subset_train = stVLE_subset_filtered[(stVLE_subset_filtered['code_presentation'] == '2013B') | \n",
        "                                              (stVLE_subset_filtered['code_presentation'] == '2013J')]\n",
        "\n",
        "    stVLE_subset_test = stVLE_subset_filtered[(stVLE_subset_filtered['code_presentation'] == '2014B') | \n",
        "                                              (stVLE_subset_filtered['code_presentation'] == '2014J')]\n",
        "  \n",
        "    stinfo_train = stInfo_update_filtered[stInfo_update_filtered['id_student'].isin(stVLE_subset_train['id_student'])]\n",
        "\n",
        "    stinfo_test = stInfo_update_filtered[stInfo_update_filtered['id_student'].isin(stVLE_subset_test['id_student'])]\n",
        "\n",
        "    \n",
        "    studentno_train = len(stVLE_subset_train['id_student'].unique())\n",
        "    \n",
        "     \n",
        "    stinfo_train['arrayIdx'] = np.arange(studentno_train)\n",
        "    \n",
        "     \n",
        "    print('the number of students {} in train course {}'.format(studentno_train, course))\n",
        "    \n",
        "    cc = stVLE_subset_train.replace({'id_site': dic_site_acitivity}, inplace=False) \n",
        "    stVLE_subset_train['activity_type'] = cc['id_site']\n",
        "    \n",
        "    cc = stVLE_subset_train.replace({'activity_type': dic_acitivity_idx}, inplace=False) \n",
        "    stVLE_subset_train['activity_idx'] = cc['activity_type']\n",
        "   \n",
        "    \n",
        "    stVLE_subset_train = pd.merge(stVLE_subset_train, stinfo_train[{'arrayIdx', 'id_student'}], on='id_student', validate='m:1')\n",
        "    \n",
        "    total_days = np.max(stVLE_subset_train['date'].values) + 1\n",
        "    \n",
        "    actionArray_train = np.zeros([studentno_train, total_days, actions_len])\n",
        "    \n",
        "    y_train = np.zeros(studentno_train)\n",
        "         \n",
        "    y_train = labelencoder.transform(stinfo_train['final_result'])\n",
        "   \n",
        "\n",
        "    for day in range(total_days):\n",
        "        \n",
        "        #print('running the course {} and the day : {}'.format(course, day))\n",
        "        \n",
        "        stVLE_days = stVLE_subset_train[stVLE_subset_train['date'] == day]\n",
        "    \n",
        "        yy = stVLE_days.groupby(['arrayIdx', 'activity_idx'])['sum_click'].sum().unstack(fill_value=0)\n",
        "\n",
        "        newyy = yy.reindex(np.arange(actions_len), axis=1, fill_value=0)\n",
        "\n",
        "        newzz = newyy.reindex(np.arange(studentno_train), axis=0, fill_value=0)\n",
        "\n",
        "        newzz = newzz.values\n",
        "\n",
        "        actionArray_train[:,day,:] = newzz\n",
        "        \n",
        "    ####\n",
        "    \n",
        "    studentno_test = len(stVLE_subset_test['id_student'].unique())\n",
        "    \n",
        "     \n",
        "    stinfo_test['arrayIdx'] = np.arange(studentno_test)\n",
        "    \n",
        "     \n",
        "    print('the number of students {} in test course {}'.format(studentno_test, course))\n",
        "    \n",
        "    cc = stVLE_subset_test.replace({'id_site': dic_site_acitivity}, inplace=False) \n",
        "    stVLE_subset_test['activity_type'] = cc['id_site']\n",
        "    \n",
        "    cc = stVLE_subset_test.replace({'activity_type': dic_acitivity_idx}, inplace=False) \n",
        "    stVLE_subset_test['activity_idx'] = cc['activity_type']\n",
        "   \n",
        "    \n",
        "    stVLE_subset_test = pd.merge(stVLE_subset_test, stinfo_test[{'arrayIdx', 'id_student'}], on='id_student', validate='m:1')\n",
        "    \n",
        "    total_days = np.max(stVLE_subset_test['date'].values) + 1\n",
        "    \n",
        "    actionArray_test= np.zeros([studentno_test, total_days, actions_len])\n",
        "    \n",
        "    y_test = np.zeros(studentno_test)\n",
        "         \n",
        "    y_test = labelencoder.transform(stinfo_test['final_result'])\n",
        "   \n",
        "    \n",
        "    for day in range(total_days):\n",
        "        \n",
        "        #print('running the course {} and the day : {}'.format(course, day))\n",
        "        \n",
        "        stVLE_days = stVLE_subset_test[stVLE_subset_test['date'] == day]\n",
        "    \n",
        "        yy = stVLE_days.groupby(['arrayIdx', 'activity_idx'])['sum_click'].sum().unstack(fill_value=0)\n",
        "\n",
        "        newyy = yy.reindex(np.arange(actions_len), axis=1, fill_value=0)\n",
        "\n",
        "        newzz = newyy.reindex(np.arange(studentno_test), axis=0, fill_value=0)\n",
        "\n",
        "        newzz = newzz.values\n",
        "\n",
        "        actionArray_test[:,day,:] = newzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq8oBXF5cvHS",
        "outputId": "3f69dff8-fe55-4731-e5af-6513724ea320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return func(*args, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of students 2790 in train course BBB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number of students 2579 in test course BBB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:90: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# merge distinction and pass \n",
        "y_train[y_train==2] = 0 \n",
        "y_test[y_test==2] = 0"
      ],
      "metadata": {
        "id": "YYFYi9achplH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[a, b] = np.unique(y_train, return_counts=True)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "[a, b] = np.unique(y_test, return_counts=True)\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODMAh9GchsKc",
        "outputId": "c13505e3-cf45-452a-aa2d-9ecef175e4b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1]\n",
            "[1899  891]\n",
            "[0 1]\n",
            "[1866  713]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# construct the data form for LSTM\n",
        "\n",
        "total_weeks = 37\n",
        "\n",
        "usedweeks = 35 # use 35-weeks' data\n",
        "\n",
        "X = list()\n",
        "\n",
        "for i in range(total_weeks):\n",
        "    seq_x = np.sum(actionArray_train[:, (7*i):(7*i+6), :], axis=1)\n",
        "    X.append(seq_x)\n",
        "X = np.array(X)\n",
        "\n",
        "actionWeeks = np.swapaxes(X, 0, 1)\n",
        "\n",
        "def prepare_data(actionWeeks, weeks=1):\n",
        "    return actionWeeks[:,:weeks,:]\n",
        "\n",
        "X_train = prepare_data(actionWeeks, weeks=usedweeks)"
      ],
      "metadata": {
        "id": "5-QjslVEh2w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = list()\n",
        "\n",
        "for i in range(total_weeks):\n",
        "    seq_x = np.sum(actionArray_test[:, (7*i):(7*i+6), :], axis=1)\n",
        "    X.append(seq_x)\n",
        "X = np.array(X)\n",
        "\n",
        "actionWeeks = np.swapaxes(X, 0, 1)\n",
        "\n",
        "def prepare_data(actionWeeks, weeks=1):\n",
        "    return actionWeeks[:,:weeks,:]\n",
        "\n",
        "X_test = prepare_data(actionWeeks, weeks=usedweeks)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "XX = np.reshape(X_train, (X_train.shape[0]*X_train.shape[1], X_train.shape[2]))\n",
        "scaler.fit(np.log(XX + 1))\n",
        "XX = scaler.transform(np.log(XX+1))\n",
        "\n",
        "X_train = np.reshape(XX, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
        "\n",
        "XX_test = np.reshape(X_test, (X_test.shape[0]*X_test.shape[1], X_test.shape[2]))\n",
        "XX_test = scaler.transform(np.log(XX_test+1))\n",
        "X_test = np.reshape(XX_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))"
      ],
      "metadata": {
        "id": "2OBoZ_lQh6W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, BatchNormalization, Bidirectional, LayerNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import regularizers\n",
        "import re\n",
        "from keras.layers import Dropout"
      ],
      "metadata": {
        "id": "E2McSW01h_WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', \n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    mode='min',\n",
        "    restore_best_weights=True)"
      ],
      "metadata": {
        "id": "nIM10KlZkWWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METRICS =[\n",
        "    keras.metrics.AUC(name ='accuracy'),\n",
        "    keras.metrics.Recall(name ='recall'),\n",
        "    keras.metrics.Precision(name = 'precision'),\n",
        "]"
      ],
      "metadata": {
        "id": "6BQs67zpkXzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = X_train.shape[1]\n",
        "n_features = X_train.shape[2]"
      ],
      "metadata": {
        "id": "Mpgew35KkrDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "from tensorflow.keras import initializers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(tf.keras.layers.Masking(mask_value=0.,\n",
        "                                   input_shape=(n_steps, n_features))) \n",
        "model.add(LSTM(300, return_sequences=False, \n",
        "               dropout=0.1, recurrent_dropout=0.1))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "# try using category instead of binary\n",
        "model.compile(loss = 'binary_crossentropy', optimizer=keras.optimizers.Adam(lr=0.01),metrics = METRICS)\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsh8_v5Kkwz_",
        "outputId": "fef9b571-7e85-4fe4-f857-dd8cd5849f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " masking (Masking)           (None, 35, 20)            0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 300)               385200    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 301       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 385,501\n",
            "Trainable params: 385,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "my_callbacks = [early_stopping]\n",
        "\n",
        "neg = len(y_train[y_train ==0])\n",
        "pos = len(y_train[y_train ==1])\n",
        "total = neg + pos\n",
        "weight_for_0 = (1 / neg)*(total)/2.0 \n",
        "weight_for_1 = (1 / pos)*(total)/2.0\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs = 100, batch_size=batch_size, \n",
        "                    validation_data=(X_test, y_test), \n",
        "                    callbacks = my_callbacks, class_weight=class_weight,\n",
        "                    verbose = 2, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fna1VgSYmC4k",
        "outputId": "ca42de1b-4df0-4291-d2d6-4f10929ca426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "28/28 - 31s - loss: 0.6586 - accuracy: 0.7410 - recall: 0.5612 - precision: 0.6188 - val_loss: 0.4419 - val_accuracy: 0.8547 - val_recall: 0.5414 - val_precision: 0.7910 - 31s/epoch - 1s/step\n",
            "Epoch 2/100\n",
            "28/28 - 31s - loss: 0.4919 - accuracy: 0.8435 - recall: 0.7082 - precision: 0.6807 - val_loss: 0.3908 - val_accuracy: 0.8677 - val_recall: 0.5708 - val_precision: 0.7903 - 31s/epoch - 1s/step\n",
            "Epoch 3/100\n",
            "28/28 - 26s - loss: 0.4758 - accuracy: 0.8516 - recall: 0.7172 - precision: 0.6783 - val_loss: 0.4631 - val_accuracy: 0.8668 - val_recall: 0.6844 - val_precision: 0.6825 - 26s/epoch - 943ms/step\n",
            "Epoch 4/100\n",
            "28/28 - 28s - loss: 0.4602 - accuracy: 0.8608 - recall: 0.7542 - precision: 0.6680 - val_loss: 0.3847 - val_accuracy: 0.8701 - val_recall: 0.5863 - val_precision: 0.8023 - 28s/epoch - 987ms/step\n",
            "Epoch 5/100\n",
            "28/28 - 26s - loss: 0.4573 - accuracy: 0.8612 - recall: 0.7419 - precision: 0.6836 - val_loss: 0.4491 - val_accuracy: 0.8690 - val_recall: 0.7139 - val_precision: 0.6751 - 26s/epoch - 934ms/step\n",
            "Epoch 6/100\n",
            "28/28 - 20s - loss: 0.4322 - accuracy: 0.8776 - recall: 0.7553 - precision: 0.6960 - val_loss: 0.4378 - val_accuracy: 0.8620 - val_recall: 0.7223 - val_precision: 0.6527 - 20s/epoch - 705ms/step\n",
            "Epoch 7/100\n",
            "28/28 - 19s - loss: 0.4341 - accuracy: 0.8758 - recall: 0.7520 - precision: 0.6994 - val_loss: 0.3974 - val_accuracy: 0.8653 - val_recall: 0.5736 - val_precision: 0.8196 - 19s/epoch - 669ms/step\n",
            "Epoch 8/100\n",
            "28/28 - 21s - loss: 0.4266 - accuracy: 0.8780 - recall: 0.7497 - precision: 0.7285 - val_loss: 0.3834 - val_accuracy: 0.8598 - val_recall: 0.5849 - val_precision: 0.8066 - 21s/epoch - 746ms/step\n",
            "Epoch 9/100\n",
            "28/28 - 21s - loss: 0.4157 - accuracy: 0.8857 - recall: 0.7520 - precision: 0.7267 - val_loss: 0.3813 - val_accuracy: 0.8664 - val_recall: 0.6283 - val_precision: 0.7943 - 21s/epoch - 763ms/step\n",
            "Epoch 10/100\n",
            "28/28 - 20s - loss: 0.4054 - accuracy: 0.8916 - recall: 0.7497 - precision: 0.7398 - val_loss: 0.4029 - val_accuracy: 0.8638 - val_recall: 0.5947 - val_precision: 0.7852 - 20s/epoch - 731ms/step\n",
            "Epoch 11/100\n",
            "28/28 - 21s - loss: 0.4115 - accuracy: 0.8881 - recall: 0.7553 - precision: 0.7299 - val_loss: 0.4457 - val_accuracy: 0.8637 - val_recall: 0.6886 - val_precision: 0.6745 - 21s/epoch - 736ms/step\n",
            "Epoch 12/100\n",
            "28/28 - 20s - loss: 0.4057 - accuracy: 0.8909 - recall: 0.7666 - precision: 0.7235 - val_loss: 0.4798 - val_accuracy: 0.8685 - val_recall: 0.7419 - val_precision: 0.6791 - 20s/epoch - 732ms/step\n",
            "Epoch 13/100\n",
            "28/28 - 23s - loss: 0.4097 - accuracy: 0.8882 - recall: 0.7553 - precision: 0.7674 - val_loss: 0.4278 - val_accuracy: 0.8628 - val_recall: 0.5820 - val_precision: 0.7950 - 23s/epoch - 813ms/step\n",
            "Epoch 14/100\n",
            "28/28 - 24s - loss: 0.3956 - accuracy: 0.8939 - recall: 0.7643 - precision: 0.7525 - val_loss: 0.4197 - val_accuracy: 0.8589 - val_recall: 0.6255 - val_precision: 0.7471 - 24s/epoch - 866ms/step\n",
            "Epoch 15/100\n",
            "28/28 - 24s - loss: 0.3909 - accuracy: 0.8979 - recall: 0.7654 - precision: 0.7503 - val_loss: 0.4263 - val_accuracy: 0.8504 - val_recall: 0.5596 - val_precision: 0.7980 - 24s/epoch - 857ms/step\n",
            "Epoch 16/100\n",
            "28/28 - 22s - loss: 0.3942 - accuracy: 0.8976 - recall: 0.7621 - precision: 0.7519 - val_loss: 0.5208 - val_accuracy: 0.8421 - val_recall: 0.6564 - val_precision: 0.6638 - 22s/epoch - 803ms/step\n",
            "Epoch 17/100\n",
            "28/28 - 23s - loss: 0.3985 - accuracy: 0.8920 - recall: 0.7486 - precision: 0.7912 - val_loss: 0.5528 - val_accuracy: 0.8404 - val_recall: 0.5652 - val_precision: 0.7491 - 23s/epoch - 830ms/step\n",
            "Epoch 18/100\n",
            "28/28 - 23s - loss: 0.3960 - accuracy: 0.8952 - recall: 0.7508 - precision: 0.7654 - val_loss: 0.5875 - val_accuracy: 0.8494 - val_recall: 0.6255 - val_precision: 0.7300 - 23s/epoch - 838ms/step\n",
            "Epoch 19/100\n",
            "Restoring model weights from the end of the best epoch: 9.\n",
            "28/28 - 21s - loss: 0.3928 - accuracy: 0.8980 - recall: 0.7587 - precision: 0.7630 - val_loss: 0.7184 - val_accuracy: 0.8287 - val_recall: 0.7419 - val_precision: 0.5707 - 21s/epoch - 753ms/step\n",
            "Epoch 19: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "ItBVi8_Wno5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SfGzYICl4BSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioBTdYeZnoY3",
        "outputId": "329a1147-093d-4a78-b7b2-4cc95c623165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81/81 [==============================] - 9s 109ms/step - loss: 0.3747 - accuracy: 0.8635 - recall: 0.5806 - precision: 0.8263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def plot_history(history, model):\n",
        "    metrics = model.metrics_names\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for n, metric in enumerate(metrics):\n",
        "        name = metric.replace(\"_\",\" \").capitalize()\n",
        "        \n",
        "        plt.subplot(3,2,n+1)\n",
        "        plt.plot(history.epoch,  history.history[metric], color='b', label='Train')\n",
        "        plt.plot(history.epoch, history.history['val_'+metric],\n",
        "             color='r', linestyle=\"--\", label='Val')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(name)\n",
        "        plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "os7TQKeDn1KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "TJN3sd4NoBah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history, model)\n",
        "\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred_train=model.predict_classes(X_train)\n",
        "con_mat = tf.math.confusion_matrix(labels=y_train, predictions=y_pred_train).numpy()\n",
        "\n",
        "cm_plot_labels = ['safe','risky']\n",
        "cm = confusion_matrix(y_true=y_train, y_pred=y_pred_train)\n",
        "\n",
        "\n",
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='training Confusion Matrix')\n",
        "\n",
        "\n",
        "y_pred_test=model.predict_classes(X_test)\n",
        "con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred_test).numpy()\n",
        "\n",
        "cm_plot_labels = ['safe','risky']\n",
        "cm = confusion_matrix(y_true=y_test, y_pred=y_pred_test)\n",
        "\n",
        "\n",
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='test Confusion Matrix')"
      ],
      "metadata": {
        "id": "JArQeFe3nuDz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}